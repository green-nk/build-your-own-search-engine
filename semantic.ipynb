{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document Loader from the workshop\n",
    "# response = requests.get(\"https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json\")\n",
    "# documents = response.json()\n",
    "# document_df = pd.json_normalize(documents, record_path=\"documents\", meta=\"course\").rename(columns={\"text\": \"answer\"}).reindex(columns=[\"course\", \"section\", \"question\", \"answer\"])\n",
    "\n",
    "# Document Loader from etl.ipynb\n",
    "with open(\"data/documents.json\") as f:\n",
    "    documents = json.load(f)\n",
    "\n",
    "document_df = pd.json_normalize(documents, record_path=\"documents\", meta=\"course\").reindex(columns=[\"course\", \"section\", \"question\", \"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    def __init__(self, fields):\n",
    "        self.fields = fields\n",
    "        self.indexes = {}\n",
    "        self.pipes = {}\n",
    "    \n",
    "    def index(self, document_df, vectorizer_params=None, embedder_params=None, embedder_name=None):\n",
    "        if vectorizer_params is None: vectorizer_params = {}\n",
    "        if embedder_params is None: embedder_params = {}\n",
    "\n",
    "        for field in self.fields:\n",
    "            vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "            embedder = TruncatedSVD(**embedder_params) if embedder_name is None or embedder_name == \"svd\" else NMF(**embedder_params)\n",
    "            pipe = Pipeline([(\"vectorizer\", vectorizer), (\"embedder\", embedder)])\n",
    "            D = pipe.fit_transform(document_df[field])\n",
    "\n",
    "            self.indexes[field] = D\n",
    "            self.pipes[field] = pipe\n",
    "        \n",
    "        self.indexes[\"document\"] = document_df\n",
    "\n",
    "    def search(self, query, k=5, weights=None, filters=None):\n",
    "        if weights is None: weights = {\"question\": 3}\n",
    "\n",
    "        if filters is None: filters = {}\n",
    "        document_df = self.indexes[\"document\"]\n",
    "        filtered_document_df = document_df[(document_df[filters.keys()] == pd.Series(filters)).all(axis=1)]\n",
    "\n",
    "        scores = np.zeros(len(filtered_document_df))\n",
    "\n",
    "        for field in self.fields:\n",
    "            w = weights.get(field, 1)\n",
    "            q = self.pipes[field].transform([query])\n",
    "\n",
    "            scores += w * cosine_similarity(q, self.indexes[field][filtered_document_df.index.values]).reshape(-1)\n",
    "\n",
    "        idx = np.argsort(-scores)[:k]\n",
    "        search_results = filtered_document_df.iloc[idx]\n",
    "\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing: Embedding via SVD\n",
    "# https://scikit-learn.org/stable/modules/decomposition.html#about-truncated-svd-and-latent-semantic-analysis-(lsa)\n",
    "engine = SemanticSearch([\"section\", \"question\", \"answer\"])\n",
    "engine.index(document_df, vectorizer_params={\"stop_words\": \"english\", \"min_df\": 3}, embedder_params={\"n_components\": 16, \"random_state\": 42})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start？',\n",
       "  'answer': \"The next cohort starts January 13th 2025. More info at DTC Article.\\nRegister before the course starts using this link.\\nJoint the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\"},\n",
       " {'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'answer': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.'},\n",
       " {'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'answer': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\"},\n",
       " {'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'answer': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\"},\n",
       " {'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - how many Zoomcamps in a year?',\n",
       "  'answer': \"There are multiple Zoomcamps in a year, as of 2025. More info at DTC Article.\\nHowever, they are for separate courses, estimated to be during these months:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nLLM (June-)\\nstock market analytics (Apr - May)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IR: Cosine Similarity\n",
    "search_results = engine.search(\n",
    "    query=\"I just signed up. Is it too late to join the course?\",\n",
    "    k=5,\n",
    "    filters={\"course\": \"data-engineering-zoomcamp\"}\n",
    ").to_dict(orient=\"records\")\n",
    "\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing: Embedding via NMF\n",
    "# https://scikit-learn.org/stable/modules/decomposition.html#non-negative-matrix-factorization-nmf-or-nnmf\n",
    "engine = SemanticSearch([\"section\", \"question\", \"answer\"])\n",
    "engine.index(document_df, vectorizer_params={\"stop_words\": \"english\", \"min_df\": 3}, embedder_params={\"n_components\": 16, \"random_state\": 42, \"max_iter\": 1000}, embedder_name=\"nmf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'answer': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\"},\n",
       " {'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start？',\n",
       "  'answer': \"The next cohort starts January 13th 2025. More info at DTC Article.\\nRegister before the course starts using this link.\\nJoint the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\"},\n",
       " {'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'answer': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.'},\n",
       " {'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'answer': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\"},\n",
       " {'course': 'data-engineering-zoomcamp',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - how many Zoomcamps in a year?',\n",
       "  'answer': \"There are multiple Zoomcamps in a year, as of 2025. More info at DTC Article.\\nHowever, they are for separate courses, estimated to be during these months:\\nData-Engineering (Jan - Apr)\\nMLOps (May - Aug)\\nMachine Learning (Sep - Jan)\\nLLM (June-)\\nstock market analytics (Apr - May)\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\\nThey follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IR: Cosine Similarity\n",
    "search_results = engine.search(\n",
    "    query=\"I just signed up. Is it too late to join the course?\",\n",
    "    k=5,\n",
    "    filters={\"course\": \"data-engineering-zoomcamp\"}\n",
    ").to_dict(orient=\"records\")\n",
    "\n",
    "search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing: Embedding via context-based representation using BERT\n",
    "# https://huggingface.co/google-bert/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(texts, batch_size=8):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "    model.eval()  # Set the model to evaluation mode if not training\n",
    "    \n",
    "    batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]\n",
    "\n",
    "    embeddings = []\n",
    "    for batch in tqdm(batches):\n",
    "        encoded_inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad(): # Disable gradient calculation for inference\n",
    "            outputs = model(**encoded_inputs)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "            batch_embeddings = hidden_states.mean(dim=1).cpu().numpy() # Mean Pooling: Aggregate the embeddings of all input tokens as sentence embedding representation\n",
    "            embeddings.append(batch_embeddings)\n",
    "\n",
    "    return np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506106a923434991ada094c3f72e5ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36880ed60934a88a8fcc17a03840347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c01b0b444b48a88584b51b55ee8fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = {field: compute_embeddings(document_df[field].tolist()) for field in [\"section\", \"question\", \"answer\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'section': array([[ 0.37748626, -0.1682663 , -0.717946  , ...,  0.32759324,\n",
       "         -0.12342941,  0.18710016],\n",
       "        [ 0.37748626, -0.1682663 , -0.717946  , ...,  0.32759324,\n",
       "         -0.12342941,  0.18710016],\n",
       "        [ 0.37748626, -0.1682663 , -0.717946  , ...,  0.32759324,\n",
       "         -0.12342941,  0.18710016],\n",
       "        ...,\n",
       "        [ 0.05408813, -0.03563844, -0.26071563, ...,  0.07740071,\n",
       "         -0.12605353,  0.06296202],\n",
       "        [ 0.05408813, -0.03563844, -0.26071563, ...,  0.07740071,\n",
       "         -0.12605353,  0.06296202],\n",
       "        [ 0.25193617,  0.06506072, -0.14671353, ..., -0.14556682,\n",
       "         -0.06536502,  0.08461889]], shape=(1086, 768), dtype=float32),\n",
       " 'question': array([[-0.07698561, -0.43341422,  0.44869146, ..., -0.07128868,\n",
       "         -0.10150252,  0.05264673],\n",
       "        [ 0.2285549 ,  0.04207362,  0.20274155, ..., -0.0888651 ,\n",
       "          0.00049368,  0.00081991],\n",
       "        [ 0.03474644, -0.27245435,  0.22815742, ...,  0.05961451,\n",
       "         -0.12863296,  0.1585731 ],\n",
       "        ...,\n",
       "        [ 0.26980466, -0.28129858,  0.01763532, ...,  0.06571149,\n",
       "          0.10106081, -0.01034799],\n",
       "        [ 0.28947017,  0.04416143,  0.11681921, ..., -0.26298475,\n",
       "          0.36863953,  0.00606511],\n",
       "        [ 0.1678844 , -0.31131074,  0.12096544, ...,  0.06143515,\n",
       "          0.04687541,  0.05224003]], shape=(1086, 768), dtype=float32),\n",
       " 'answer': array([[ 0.22671555, -0.2847486 ,  0.5767982 , ..., -0.05312702,\n",
       "         -0.0075148 ,  0.02594928],\n",
       "        [-0.05745921, -0.144339  ,  0.06166077, ..., -0.10502662,\n",
       "         -0.10845073,  0.17458084],\n",
       "        [ 0.21461241, -0.0766345 ,  0.29986528, ...,  0.1159509 ,\n",
       "         -0.06826312, -0.02452073],\n",
       "        ...,\n",
       "        [ 0.23040776, -0.21982615,  0.28237775, ...,  0.26106232,\n",
       "          0.17824832, -0.05426816],\n",
       "        [ 0.02622777, -0.05092067,  0.13299976, ..., -0.11876699,\n",
       "          0.16964635,  0.08734052],\n",
       "        [ 0.10018042,  0.01447496,  0.31222925, ...,  0.09778161,\n",
       "          0.17682433,  0.13937312]], shape=(1086, 768), dtype=float32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086, 768)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[\"answer\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real world, the most applicable approach to do information retrieval is to combine both keyword search and semantic search together so called *Hybrid Search*. The retrieved results are calculated based on weighted scores on both systems.\n",
    "\n",
    "Moreover, in practice there are a massive set of documents paired with just a query that we need to compute for similarity scores. We can not do efficient real-time querying despite preparing documents before hand or using good algorithm when retrieving. Therefore, we need to optimize each step so that we can build an effective IR system.\n",
    "\n",
    "In IR, a task is simply that:\n",
    "> Given a query q, find a set of relevant documents D.\n",
    "\n",
    "There are two main steps involved in the system. The first one is **ETL**, which is a process to do data ingestion to build 'external' knowledge (i.e., database). This process is analogous to data engineering process. The most common techniques in this stage for key word search is *inverted index* which is a map of word associated with its posting lists. A posting list can contain various information such as DocID, position in the document and the occurrence of the word. The data structure allows fast retrieval of postings given terms in the query.\n",
    "\n",
    "Next step is **Searching**. The main goal is to calculate a similarity score for each document-query pair via an algorithm like k-nearest neighbors. This is very inefficient to compute when we do not know the query in advance.\n",
    "\n",
    "One way to mitigate this is to pre-compute its k-nearest neighbors documents for each term before hand. However, it means we have to store additional data in the database and waste our storage for duplicate data. The more common technique is to use *approximate nearest neighbors (ANN)* specifically for IR, *Locality-Sensitive Hashing (LSH) via random projections*, which is just a hash function for retrieving similar items that in the same bucket.\n",
    "\n",
    "Note that those are algorithms and data structures implemented in the services that we can apply in our application. We do not have to build it from scratch. For example, in key word search, we can use *Elastic Search* to implement production-ready search system. Also, we have a lot of on-the-shelf vector databases like *Pinecone* or *FAISS* for implementing semantic search or hybrid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
